{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2022-08-25T11:47:24.910722Z","iopub.execute_input":"2022-08-25T11:47:24.911176Z","iopub.status.idle":"2022-08-25T11:47:25.408554Z","shell.execute_reply.started":"2022-08-25T11:47:24.911082Z","shell.execute_reply":"2022-08-25T11:47:25.407413Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"### Reading the tweet Dataset","metadata":{}},{"cell_type":"code","source":"df_tweet = pd.read_csv(\"../input/tweet-mental-health-classification/train.csv\")\ndf_tweet.shape","metadata":{"execution":{"iopub.status.busy":"2022-08-25T11:47:25.411031Z","iopub.execute_input":"2022-08-25T11:47:25.411455Z","iopub.status.idle":"2022-08-25T11:47:25.455553Z","shell.execute_reply.started":"2022-08-25T11:47:25.411416Z","shell.execute_reply":"2022-08-25T11:47:25.454406Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"### Let us look at the class distribution","metadata":{}},{"cell_type":"code","source":"df_tweet['labels'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-08-25T11:47:25.457413Z","iopub.execute_input":"2022-08-25T11:47:25.457809Z","iopub.status.idle":"2022-08-25T11:47:25.469338Z","shell.execute_reply.started":"2022-08-25T11:47:25.457771Z","shell.execute_reply":"2022-08-25T11:47:25.468285Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"#### Since it was observed that Anxious and lonely are higly correlated. We ll try to combine Anxious and lonely as common class","metadata":{}},{"cell_type":"code","source":"df_tweet.loc[df_tweet['labels'].isin(['Anxious',\"Lonely\"]),'labels'] = \"Anx_Lonely\"\ndf_tweet = df_tweet.drop_duplicates()\ndf_tweet['labels'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-08-25T11:47:25.472455Z","iopub.execute_input":"2022-08-25T11:47:25.473348Z","iopub.status.idle":"2022-08-25T11:47:25.501197Z","shell.execute_reply.started":"2022-08-25T11:47:25.473307Z","shell.execute_reply":"2022-08-25T11:47:25.500313Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"### convert all the text into lower case and convert labels int numeric valuse","metadata":{}},{"cell_type":"code","source":"df_tweet['tweets'] = df_tweet['tweets'].apply(lambda x: x.lower())\n\nmapper = {\n    \"Normal\": 0,\n    \"Anx_Lonely\": 1,\n    \"Stressed\": 2\n}\n\ndf_tweet['labels'] = df_tweet[\"labels\"].map(mapper)\ndf_tweet.shape","metadata":{"execution":{"iopub.status.busy":"2022-08-25T11:47:25.502695Z","iopub.execute_input":"2022-08-25T11:47:25.503059Z","iopub.status.idle":"2022-08-25T11:47:25.522972Z","shell.execute_reply.started":"2022-08-25T11:47:25.503023Z","shell.execute_reply":"2022-08-25T11:47:25.521815Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"### Preparing training and testing Data","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom transformers import AutoTokenizer, TFBertModel\nimport tensorflow as tf\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.layers import Dense, Dropout\n\ndf_train, df_test = train_test_split(df_tweet,train_size=0.75, random_state=2)","metadata":{"execution":{"iopub.status.busy":"2022-08-25T11:47:25.524416Z","iopub.execute_input":"2022-08-25T11:47:25.524686Z","iopub.status.idle":"2022-08-25T11:47:30.483493Z","shell.execute_reply.started":"2022-08-25T11:47:25.524659Z","shell.execute_reply":"2022-08-25T11:47:30.482394Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"### Preparing Batch Data ","metadata":{}},{"cell_type":"code","source":"def num_batches(total, batch_size):\n    if total % batch_size == 0:\n        return total // batch_size\n    else:\n        return total // batch_size + 1\n\n\n\ndef generate_batch(df,text_col,label_col,batch_size=150):\n    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\",\n                                              padding_side='left',truncation_side='right')\n    #model = TFBertModel.from_pretrained(\"bert-base-uncased\")\n    texts = df[text_col].tolist()\n    unique_labels = len(df[label_col].unique())\n    labels = np.asarray(df[label_col])\n    total = df.shape[0]\n    batches = num_batches(total, batch_size)\n    \n    while True:\n        for b in range(batches):\n\n            if total % batch_size == 0 or b < batches - 1:\n                y_data = np.zeros((batch_size,unique_labels))\n                batch_labels = labels[b*batch_size : b*batch_size + batch_size]\n                batch_texts = texts[b*batch_size : b*batch_size + batch_size]\n                tokens = tokenizer(batch_texts,max_length=25,padding='max_length',\n                                   truncation=True,return_tensors='tf')\n                for i in range(batch_size):\n                    cat = batch_labels[i]\n                    y_data[i,cat] = 1\n            else:\n                y_data = np.zeros((total % batch_size,unique_labels))\n                batch_labels = labels[b*batch_size : b*batch_size + total % batch_size]\n                batch_texts = texts[b*batch_size : b*batch_size + total % batch_size]\n                tokens = tokenizer(batch_texts,max_length=25,padding='max_length',\n                                   truncation=True,return_tensors='tf')\n                for i in range(total % batch_size):\n                    cat = batch_labels[i]\n                    y_data[i,cat] = 1\n\n            \n            t1 = tokens['input_ids'] = tokens['input_ids'].numpy()\n            t2 = tokens['token_type_ids'] = tokens['token_type_ids'].numpy()\n            t3 = tokens['attention_mask'] = tokens['attention_mask'].numpy()\n            #print(t2.shape,y_data.shape)\n            yield (t1,t2,t3), y_data","metadata":{"execution":{"iopub.status.busy":"2022-08-25T11:47:30.485152Z","iopub.execute_input":"2022-08-25T11:47:30.486067Z","iopub.status.idle":"2022-08-25T11:47:30.500065Z","shell.execute_reply.started":"2022-08-25T11:47:30.486024Z","shell.execute_reply":"2022-08-25T11:47:30.499026Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### Let us look at the tokens","metadata":{}},{"cell_type":"markdown","source":"### Building A Tensorflow BERT model\n- The model will be fine tuned","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.layers import Input\nfrom tensorflow.keras.optimizers import Adam\n\nbert_model = TFBertModel.from_pretrained(\"bert-base-uncased\")\n\ninput_ids = Input(shape=25,dtype=np.int64)\ntoken_type_ids =  Input(shape=25,dtype=np.int64)\nattention_mask = Input(shape=25,dtype=np.int64)\nbert_out = bert_model([input_ids,token_type_ids,attention_mask])\ndense = Dense(3,activation='softmax')\nout = dense(bert_out.last_hidden_state[:,0,:])\n\nmodel = Model([input_ids,token_type_ids,attention_mask],out)\n\nmodel.compile(loss='categorical_crossentropy',optimizer=Adam(2e-6),metrics='accuracy')\nmodel.summary()\n","metadata":{"execution":{"iopub.status.busy":"2022-08-25T11:47:30.501834Z","iopub.execute_input":"2022-08-25T11:47:30.502194Z","iopub.status.idle":"2022-08-25T11:47:38.775023Z","shell.execute_reply.started":"2022-08-25T11:47:30.502155Z","shell.execute_reply":"2022-08-25T11:47:38.774012Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### Fitting the model","metadata":{}},{"cell_type":"code","source":"train_gen  = generate_batch(df_train,'tweets','labels',150)\nval_gen = generate_batch(df_test,'tweets','labels',150)\nsteps_per_epoch = num_batches(df_train.shape[0],150)\nval_steps = num_batches(df_test.shape[0],150)\n\nhistory = model.fit_generator(train_gen,steps_per_epoch=steps_per_epoch,\n                              validation_data=val_gen,validation_steps=val_steps,epochs=15)","metadata":{"execution":{"iopub.status.busy":"2022-08-25T11:47:38.780002Z","iopub.execute_input":"2022-08-25T11:47:38.783184Z","iopub.status.idle":"2022-08-25T11:58:20.609946Z","shell.execute_reply.started":"2022-08-25T11:47:38.783143Z","shell.execute_reply":"2022-08-25T11:58:20.608801Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"h = history.history\nplt.figure(figsize=(10,8))\nepochs = 15\nplt.subplot(1,2,1)\nplt.plot(range(1,epochs+1),h['loss'])\nplt.plot(range(1,epochs+1),h['val_loss'])\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.subplot(1,2,2)\nplt.plot(range(1,epochs+1),h['accuracy'])\nplt.plot(range(1,epochs+1),h['val_accuracy'])\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Accuracy\")","metadata":{"execution":{"iopub.status.busy":"2022-08-25T11:58:20.615095Z","iopub.execute_input":"2022-08-25T11:58:20.615452Z","iopub.status.idle":"2022-08-25T11:58:20.940230Z","shell.execute_reply.started":"2022-08-25T11:58:20.615421Z","shell.execute_reply":"2022-08-25T11:58:20.939243Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"### Saved tuned weightsBERT weights\n`model.layers[-3]` is the bert model layer","metadata":{}},{"cell_type":"code","source":"import os\nimport shutil\nshutil.rmtree(\"model\")\nos.mkdir(\"model\")\nb_layer = model.layers[-3]\nb_layer.save_pretrained(\"model/bert-tuned\")","metadata":{"execution":{"iopub.status.busy":"2022-08-25T11:58:20.942373Z","iopub.execute_input":"2022-08-25T11:58:20.943114Z","iopub.status.idle":"2022-08-25T11:58:21.964624Z","shell.execute_reply.started":"2022-08-25T11:58:20.943072Z","shell.execute_reply":"2022-08-25T11:58:21.963586Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"### Evaluating the data on test set","metadata":{}},{"cell_type":"code","source":"\ndef get_test_data(df,text_col,label_col,batch_size=150):\n    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\",\n                                              padding_side='left',truncation_side='right')\n    #model = TFBertModel.from_pretrained(\"bert-base-uncased\")\n    texts = df[text_col].tolist()\n    unique_labels = len(df[label_col].unique())\n    labels = np.asarray(df[label_col])\n    total = df.shape[0]\n    batches = num_batches(total, batch_size)\n    t1 = np.zeros((total,25))\n    t2 = np.zeros((total,25))\n    t3 = np.zeros((total,25))\n    y_data = np.zeros((total,unique_labels))\n    for b in range(batches):\n\n        if total % batch_size == 0 or b < batches - 1:\n            \n            batch_labels = labels[b*batch_size : b*batch_size + batch_size]\n            batch_texts = texts[b*batch_size : b*batch_size + batch_size]\n            tokens = tokenizer(batch_texts,max_length=25,padding='max_length',\n                               truncation=True,return_tensors='tf')\n            \n            t1[b*batch_size:b*batch_size+batch_size] = tokens['input_ids'].numpy()\n            t2[b*batch_size:b*batch_size+batch_size] = tokens['token_type_ids'].numpy()\n            t3[b*batch_size:b*batch_size+batch_size] = tokens['attention_mask'].numpy()\n            for i in range(batch_size):\n                cat = batch_labels[i]\n                y_data[b*batch_size + i,cat] = 1\n        else:\n            \n            batch_labels = labels[b*batch_size : b*batch_size + total % batch_size]\n            batch_texts = texts[b*batch_size : b*batch_size + total % batch_size]\n            tokens = tokenizer(batch_texts,max_length=25,padding='max_length',\n                               truncation=True,return_tensors='tf')\n            t1[b*batch_size:] = tokens['input_ids'].numpy()\n            t2[b*batch_size:] = tokens['token_type_ids'].numpy()\n            t3[b*batch_size:] = tokens['attention_mask'].numpy()\n           \n            for i in range(total % batch_size):\n                cat = batch_labels[i]\n                y_data[b*batch_size + i,cat] = 1\n\n\n        \n        #print(t2.shape,y_data.shape)\n    return (t1,t2,t3), y_data","metadata":{"execution":{"iopub.status.busy":"2022-08-25T11:58:21.966432Z","iopub.execute_input":"2022-08-25T11:58:21.966818Z","iopub.status.idle":"2022-08-25T11:58:21.981269Z","shell.execute_reply.started":"2022-08-25T11:58:21.966780Z","shell.execute_reply":"2022-08-25T11:58:21.979968Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"### Prediction on test set","metadata":{}},{"cell_type":"code","source":"test_data = get_test_data(df_test,'tweets','labels')\ninputs = test_data[0]\npredictions = model.predict(inputs)","metadata":{"execution":{"iopub.status.busy":"2022-08-25T11:58:21.983024Z","iopub.execute_input":"2022-08-25T11:58:21.983760Z","iopub.status.idle":"2022-08-25T11:58:36.035308Z","shell.execute_reply.started":"2022-08-25T11:58:21.983671Z","shell.execute_reply":"2022-08-25T11:58:36.034130Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"y_test = df_test['labels']\ny_test_pred = np.argmax(predictions,axis=1)\ncf = confusion_matrix(y_test,y_test_pred)\ncf","metadata":{"execution":{"iopub.status.busy":"2022-08-25T11:58:36.037376Z","iopub.execute_input":"2022-08-25T11:58:36.037811Z","iopub.status.idle":"2022-08-25T11:58:36.049923Z","shell.execute_reply.started":"2022-08-25T11:58:36.037763Z","shell.execute_reply":"2022-08-25T11:58:36.048603Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test,y_test_pred))","metadata":{"execution":{"iopub.status.busy":"2022-08-25T11:58:36.051880Z","iopub.execute_input":"2022-08-25T11:58:36.052517Z","iopub.status.idle":"2022-08-25T11:58:36.071086Z","shell.execute_reply.started":"2022-08-25T11:58:36.052477Z","shell.execute_reply":"2022-08-25T11:58:36.069938Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"### BERT + LSTM hybrid","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.layers import Bidirectional, LSTM\n\n","metadata":{"execution":{"iopub.status.busy":"2022-08-25T11:58:36.072736Z","iopub.execute_input":"2022-08-25T11:58:36.073383Z","iopub.status.idle":"2022-08-25T11:58:36.078666Z","shell.execute_reply.started":"2022-08-25T11:58:36.073340Z","shell.execute_reply":"2022-08-25T11:58:36.077332Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"### Step 1 is to convert all the texts into embeddings\n### The Embeddings are generated from the fine-tuned BERT weights","metadata":{}},{"cell_type":"code","source":"### Load saved model\nhyb_model = TFBertModel.from_pretrained(\"model/bert-tuned\")\ndef load_embeddings(text_data,batch_size=150,model=hyb_model):\n    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\",padding_side='left',\n                                             truncation_side='right')\n    total = len(text_data)\n    batches = num_batches(total,batch_size)\n    features = np.zeros((total,25,768))\n    for b in range(batches):\n        if total % batch_size == 0 or b < batches -1:\n            batch_texts = text_data[b*batch_size:b*batch_size+batch_size]\n            tokens = tokenizer(batch_texts,max_length=25,padding=\"max_length\",\n                               truncation=True, return_tensors=\"tf\")\n            embeddings = model(tokens).last_hidden_state\n            features[b*batch_size:b*batch_size+batch_size,:,:] = embeddings\n        else:\n            batch_texts = text_data[b*batch_size:]\n            tokens = tokenizer(batch_texts,max_length=25,padding=\"max_length\",\n                               truncation=True, return_tensors=\"tf\")\n            embeddings = model(tokens).last_hidden_state\n            features[b*batch_size:,:,:] = embeddings\n    return features\n            ","metadata":{"execution":{"iopub.status.busy":"2022-08-25T11:58:36.080665Z","iopub.execute_input":"2022-08-25T11:58:36.081479Z","iopub.status.idle":"2022-08-25T11:58:37.225623Z","shell.execute_reply.started":"2022-08-25T11:58:36.081440Z","shell.execute_reply":"2022-08-25T11:58:37.224670Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"texts_train = df_train['tweets'].tolist()\ntexts_test = df_test['tweets'].tolist()\ntrain_data = load_embeddings(texts_train,batch_size=50,model=hyb_model)\ntest_data = load_embeddings(texts_test,batch_size=50,model=hyb_model)","metadata":{"execution":{"iopub.status.busy":"2022-08-25T11:58:37.226997Z","iopub.execute_input":"2022-08-25T11:58:37.228070Z","iopub.status.idle":"2022-08-25T11:59:23.982125Z","shell.execute_reply.started":"2022-08-25T11:58:37.228029Z","shell.execute_reply":"2022-08-25T11:59:23.981096Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"train_data.shape","metadata":{"execution":{"iopub.status.busy":"2022-08-25T11:59:23.984410Z","iopub.execute_input":"2022-08-25T11:59:23.984791Z","iopub.status.idle":"2022-08-25T11:59:23.993107Z","shell.execute_reply.started":"2022-08-25T11:59:23.984752Z","shell.execute_reply":"2022-08-25T11:59:23.991935Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import Sequential\nlstm_model = Sequential()\nlstm_model.add(Bidirectional(LSTM(256),input_shape=(25,768)))\nlstm_model.add(Dropout(0.1))\nlstm_model.add(Dense(3,activation='softmax'))\n\nlstm_model.compile(optimizer=\"adam\",loss='sparse_categorical_crossentropy',metrics='accuracy')\nlstm_model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-08-25T12:00:42.353783Z","iopub.execute_input":"2022-08-25T12:00:42.355021Z","iopub.status.idle":"2022-08-25T12:00:42.834055Z","shell.execute_reply.started":"2022-08-25T12:00:42.354975Z","shell.execute_reply":"2022-08-25T12:00:42.831331Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"y_train = df_train['labels']\ny_test = df_test['labels']\nhistory = lstm_model.fit(train_data,y_train,batch_size=150,epochs=10,\n               validation_data=(test_data,y_test))","metadata":{"execution":{"iopub.status.busy":"2022-08-25T12:00:42.836362Z","iopub.execute_input":"2022-08-25T12:00:42.837500Z","iopub.status.idle":"2022-08-25T12:01:07.624236Z","shell.execute_reply.started":"2022-08-25T12:00:42.837457Z","shell.execute_reply":"2022-08-25T12:01:07.623136Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"### Plotting the model","metadata":{}},{"cell_type":"code","source":"h = history.history\nplt.figure(figsize=(10,6))\nepochs = 10\nplt.subplot(1,2,1)\nplt.plot(range(1,epochs+1),h['loss'])\nplt.plot(range(1,epochs+1),h['val_loss'])\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.subplot(1,2,2)\nplt.plot(range(1,epochs+1),h['accuracy'])\nplt.plot(range(1,epochs+1),h['val_accuracy'])\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Accuracy\")","metadata":{"execution":{"iopub.status.busy":"2022-08-25T12:02:29.684552Z","iopub.execute_input":"2022-08-25T12:02:29.685035Z","iopub.status.idle":"2022-08-25T12:02:29.993743Z","shell.execute_reply.started":"2022-08-25T12:02:29.684994Z","shell.execute_reply":"2022-08-25T12:02:29.992655Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"### No Significant improvement by this experiment","metadata":{}},{"cell_type":"markdown","source":"### Generating the classification report","metadata":{}},{"cell_type":"code","source":"y_test_pred = lstm_model.predict(test_data)\ny_test_pred = np.argmax(y_test_pred,axis=1)\ncf = confusion_matrix(y_test,y_test_pred)\nprint(cf)","metadata":{"execution":{"iopub.status.busy":"2022-08-25T12:04:44.512239Z","iopub.execute_input":"2022-08-25T12:04:44.513057Z","iopub.status.idle":"2022-08-25T12:04:46.212076Z","shell.execute_reply.started":"2022-08-25T12:04:44.513016Z","shell.execute_reply":"2022-08-25T12:04:46.210971Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test,y_test_pred))","metadata":{"execution":{"iopub.status.busy":"2022-08-25T12:05:13.263063Z","iopub.execute_input":"2022-08-25T12:05:13.264304Z","iopub.status.idle":"2022-08-25T12:05:13.281762Z","shell.execute_reply.started":"2022-08-25T12:05:13.264227Z","shell.execute_reply":"2022-08-25T12:05:13.280526Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}