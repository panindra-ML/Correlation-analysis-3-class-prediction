{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import TFBertModel, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15176, 2)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets = pd.read_csv(\"train.csv\")\n",
    "\n",
    "### Label 0 == \"anxious\" and Label 3 == \"Lonely\"\n",
    "### All the models are confused between anxious and lonely\n",
    "df_tweets = df_tweets[df_tweets['labels'].isin([\"Anxious\",\"Lonely\"])]\n",
    "df_tweets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will use the fine tuned version of the BERT model to generate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertModel.\n",
      "\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at C:\\Users\\raoms_y121yee\\Downloads\\tuned-model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tuned_model_path = r\"C:\\Users\\raoms_y121yee\\Downloads\\tuned-model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\",padding_side=\"left\",truncation_side=\"right\")\n",
    "bert_model = TFBertModel.from_pretrained(tuned_model_path)\n",
    "\n",
    "\n",
    "def num_batches(total,batch_size):\n",
    "    if total % batch_size == 0:\n",
    "        return total // batch_size\n",
    "    else:\n",
    "        return total // batch_size + 1\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "def generate_embeddings(texts,batch_size,tokenizer=tokenizer,model=bert_model):\n",
    "    total = len(texts)\n",
    "    batches = num_batches(total,batch_size)\n",
    "    embeddings = np.zeros((total,768))\n",
    "    for b in range(batches):\n",
    "        if total % batch_size == 0 or b < batches -1 :\n",
    "            batch_texts = texts[b*batch_size:b*batch_size + batch_size]\n",
    "            tokens = tokenizer(batch_texts,return_tensors=\"tf\",max_length=20,padding=True,truncation=True)\n",
    "            e = model(tokens).last_hidden_state[:,0,:].numpy()\n",
    "            embeddings[b*batch_size:b*batch_size+batch_size, :] = e\n",
    "        else:\n",
    "            batch_texts = texts[b*batch_size:]\n",
    "            tokens = tokenizer(batch_texts,return_tensors=\"tf\",max_length=20,padding=True,truncation=True)\n",
    "            e = model(tokens).last_hidden_state[:,0,:].numpy()\n",
    "            embeddings[b*batch_size:, :] = e\n",
    "            \n",
    "    return embeddings\n",
    "\n",
    "\n",
    "##### Generate Embeddings here\n",
    "anxious = df_tweets.loc[df_tweets['labels'] == 'Anxious','tweets'].tolist()\n",
    "lonely = df_tweets.loc[df_tweets['labels'] == 'Lonely','tweets'].tolist()\n",
    "anxious_emb = generate_embeddings(anxious,150)\n",
    "lonely_emb = generate_embeddings(lonely,150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.66682042, 0.70910269, 0.62089508, ..., 0.68441209, 0.62252213,\n",
       "        0.59645158],\n",
       "       [0.64943828, 0.71428288, 0.60574092, ..., 0.62951312, 0.64138027,\n",
       "        0.70752305],\n",
       "       [0.64712961, 0.70815988, 0.65281628, ..., 0.65333793, 0.55360437,\n",
       "        0.6880034 ],\n",
       "       ...,\n",
       "       [0.57163513, 0.59779753, 0.64126983, ..., 0.54625021, 0.57558718,\n",
       "        0.68623517],\n",
       "       [0.58568226, 0.71666109, 0.57453597, ..., 0.69081999, 0.53234898,\n",
       "        0.57697925],\n",
       "       [0.7400193 , 0.70203316, 0.61149947, ..., 0.64758908, 0.67233236,\n",
       "        0.68118443]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "sm = 1 - pairwise_distances(anxious_emb,lonely_emb,metric=\"cosine\")\n",
    "sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_similar_index = np.argmax(sm,axis=1)\n",
    "most_sim_score = np.max(sm,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.84670154, 0.86240447, ..., 0.83400858, 1.        ,\n",
       "       1.        ])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_sim_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "lon = pd.Series(lonely)\n",
    "lon = lon.iloc[most_similar_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = pd.DataFrame({\n",
    "    \"Anxious\": anxious,\n",
    "    \"Lonely\": lon,\n",
    "    \"cosine_score\": most_sim_score\n",
    "})\n",
    "\n",
    "final = res_df[res_df['cosine_score'] < 1]\n",
    "final.to_csv(\"Similar.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2120, 3)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
